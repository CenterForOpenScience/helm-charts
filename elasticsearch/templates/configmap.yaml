apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "elasticsearch.fullname" . }}
  labels:
    app: {{ template "elasticsearch.fullname" . }}
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    release: "{{ .Release.Name }}"
    heritage: "{{ .Release.Service }}"
data:
  elasticsearch.yml: |-
    cluster.name: {{ .Values.cluster.name }}

    node.data: ${NODE_DATA:true}
    node.master: ${NODE_MASTER:true}
    {{- if semverCompare ">= 5.x" .Values.appVersion }}
    node.ingest: ${NODE_INGEST:true}
    {{- end }}
    node.name: ${HOSTNAME}

    network.host: 0.0.0.0

    {{- if semverCompare "^2.x" .Values.appVersion }}
    # see https://github.com/kubernetes/kubernetes/issues/3595
    bootstrap.mlockall: ${BOOTSTRAP_MLOCKALL:false}
    {{- end }}
    {{- if semverCompare ">= 5.x" .Values.appVersion }}
    # see https://github.com/kubernetes/kubernetes/issues/3595
    bootstrap.memory_lock: ${BOOTSTRAP_MEMORY_LOCK:false}
    {{- end }}

    discovery:
      zen:
        ping.unicast.hosts: ${DISCOVERY_SERVICE:}
        minimum_master_nodes: ${MINIMUM_MASTER_NODES:2}

    # see https://github.com/elastic/elasticsearch-definitive-guide/pull/679
    processors: ${PROCESSORS:}

    # avoid split-brain w/ a minimum consensus of two masters plus a data node
    gateway.expected_master_nodes: ${EXPECTED_MASTER_NODES:2}
    gateway.expected_data_nodes: ${EXPECTED_DATA_NODES:1}
    gateway.recover_after_time: ${RECOVER_AFTER_TIME:5m}
    gateway.recover_after_master_nodes: ${RECOVER_AFTER_MASTER_NODES:2}
    gateway.recover_after_data_nodes: ${RECOVER_AFTER_DATA_NODES:1}

    # Extra Configuration
    {{- if .Values.cluster.config }}
    {{- toYaml .Values.cluster.config | nindent 4 }}
    {{- end }}

    # X-Pack
    {{- if and .Values.xpack.enabled .Values.xpack.config }}
    {{- toYaml .Values.xpack.config | nindent 4 }}
    {{- end }}

    # Search Guard
    {{- if and .Values.searchguard.enabled .Values.searchguard.config }}
    {{- toYaml .Values.searchguard.config | nindent 4 }}
    {{- end }}
  {{- if semverCompare "^2.x" .Values.appVersion }}
  logging.yml: |-
    # you can override this using by setting a system property, for example -Des.logger.level=DEBUG
    es.logger.level: INFO
    rootLogger: ${es.logger.level}, console
    logger:
      # log action execution errors for easier debugging
      action: DEBUG
      # reduce the logging for aws, too much is logged under the default INFO
      com.amazonaws: WARN

    appender:
      console:
        type: console
        layout:
          type: consolePattern
          conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"
  {{- end }}
  {{- if semverCompare ">= 5.x" .Values.appVersion }}
  log4j2.properties: |-
    status = error

    appender.console.type = Console
    appender.console.name = console
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n

    rootLogger.level = info
    rootLogger.appenderRef.console.ref = console

    {{- if .Values.searchguard.enabled }}
    logger.searchguard.name = com.floragunn
    logger.searchguard.level = info
    {{- end }}
  {{- end }}
  pre-stop-hook.sh: |-
    #!/bin/bash
    set -e

    SERVICE_ACCOUNT_PATH=/var/run/secrets/kubernetes.io/serviceaccount
    KUBE_TOKEN=$(<${SERVICE_ACCOUNT_PATH}/token)
    KUBE_NAMESPACE=$(<${SERVICE_ACCOUNT_PATH}/namespace)

    STATEFULSET_NAME=$(echo "${HOSTNAME}" | sed 's/-[0-9]*$//g')
    INSTANCE_ID=$(echo "${HOSTNAME}" | grep -o '[0-9]*$')

    echo "Prepare stopping of Pet ${KUBE_NAMESPACE}/${HOSTNAME} of StatefulSet ${KUBE_NAMESPACE}/${STATEFULSET_NAME} instance_id ${INSTANCE_ID}"

    export STATEFULSET_STATUS=$(
      curl -s \
        --cacert ${SERVICE_ACCOUNT_PATH}/ca.crt \
        -H "Authorization: Bearer $KUBE_TOKEN" \
        "https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_PORT_443_TCP_PORT}/apis/apps/v1/namespaces/${KUBE_NAMESPACE}/statefulsets/${STATEFULSET_NAME}/status"
    )
    INSTANCES_DESIRED=$(
    	python - <<-EOF
    		import json
    		import os
    
    		obj = json.loads(os.environ.get('STATEFULSET_STATUS'))
    		print(obj['spec']['replicas'])
    	EOF
    )

    echo "Desired instance count is ${INSTANCES_DESIRED}"

    if [ "${INSTANCE_ID}" -lt "${INSTANCES_DESIRED}" ]; then
      echo "No data migration needed"
      exit 0
    fi

    echo "Prepare to migrate data of the node"

    export NODE_STATS=$(
      curl -X GET -s \
        {{- if .Values.tls.enabled }}
        --insecure \
        --crlfile /usr/share/elasticsearch/config/ca_crl.pem \
        --key /usr/share/elasticsearch/config/server_key.pem \
        --cacert /usr/share/elasticsearch/config/ca_cert.pem \
        --cert /usr/share/elasticsearch/config/server_cert.pem \
        {{- end }}
        http{{ if .Values.tls.enabled }}s{{ end }}://127.0.0.1:9200/_nodes/stats
    )
    NODE_IP=$(
    	python - <<-EOF
    		import json
    		import os
    
    		obj = json.loads(os.environ.get('NODE_STATS'))
    		key = list(filter(lambda datum: obj['nodes'][datum]['name'] == os.environ.get('HOSTNAME'), obj['nodes'].keys()))[0]
    		node = obj['nodes'][key]
    		print(node['host'])
    	EOF
    )

    echo "Move all data from node ${NODE_IP}"

    curl -X PUT -H "Content-Type: application/json" -s \
      {{- if .Values.tls.enabled }}
      --insecure \
      --crlfile /usr/share/elasticsearch/config/ca_crl.pem \
      --key /usr/share/elasticsearch/config/server_key.pem \
      --cacert /usr/share/elasticsearch/config/ca_cert.pem \
      --cert /usr/share/elasticsearch/config/server_cert.pem \
      {{- end }}
      http{{ if .Values.tls.enabled }}s{{ end }}://127.0.0.1:9200/_cluster/settings \
      --data "{
          \"transient\" :{
              \"cluster.routing.allocation.exclude._ip\" : \"${NODE_IP}\"
          }
        }"
    echo

    echo "Wait for node documents to become empty"
    DOC_COUNT=$(
    	python - <<-EOF
    		import json
    		import os
    
    		obj = json.loads(os.environ.get('NODE_STATS'))
    		key = list(filter(lambda datum: obj['nodes'][datum]['name'] == os.environ.get('HOSTNAME'), obj['nodes'].keys()))[0]
    		node = obj['nodes'][key]
    		print(node['indices']['docs']['count'])
    	EOF
    )

    while [ "${DOC_COUNT}" -gt 0 ]; do
      export NODE_STATS=$(
        curl -X GET -s \
          {{- if .Values.tls.enabled }}
          --insecure \
          --crlfile /usr/share/elasticsearch/config/ca_crl.pem \
          --key /usr/share/elasticsearch/config/server_key.pem \
          --cacert /usr/share/elasticsearch/config/ca_cert.pem \
          --cert /usr/share/elasticsearch/config/server_cert.pem \
          {{- end }}
          http{{ if .Values.tls.enabled }}s{{ end }}://127.0.0.1:9200/_nodes/stats
      )
      DOC_COUNT=$(
    		python - <<-EOF
    			import json
    			import os
    
    			obj = json.loads(os.environ.get('NODE_STATS'))
    			key = list(filter(lambda datum: obj['nodes'][datum]['name'] == os.environ.get('HOSTNAME'), obj['nodes'].keys()))[0]
    			node = obj['nodes'][key]
    			count = node['indices']['docs']['count']
    			print(count)
    		EOF
      )
      echo "Node contains ${DOC_COUNT} documents"
      sleep 1
    done

    echo "Wait for node shards to become empty"
    export SHARD_STATS=$(
      curl -X GET -s \
        {{- if .Values.tls.enabled }}
        --insecure \
        --crlfile /usr/share/elasticsearch/config/ca_crl.pem \
        --key /usr/share/elasticsearch/config/server_key.pem \
        --cacert /usr/share/elasticsearch/config/ca_cert.pem \
        --cert /usr/share/elasticsearch/config/server_cert.pem \
        {{- end }}
        http{{ if .Values.tls.enabled }}s{{ end }}://127.0.0.1:9200/_cat/shards?format=json
    )
    SHARD_COUNT=$(
    	python - <<-EOF
    		import json
    		import os
    
    		obj = json.loads(os.environ.get('SHARD_STATS'))
    		count = len(filter(lambda datum: datum['node'] == os.environ.get('HOSTNAME'), obj))
    		print(count)
    	EOF
    )
    while [ "${SHARD_COUNT}" -gt 0 ]; do
      export SHARD_STATS=$(
        curl -X GET -s \
          {{- if .Values.tls.enabled }}
          --insecure \
          --crlfile /usr/share/elasticsearch/config/ca_crl.pem \
          --key /usr/share/elasticsearch/config/server_key.pem \
          --cacert /usr/share/elasticsearch/config/ca_cert.pem \
          --cert /usr/share/elasticsearch/config/server_cert.pem \
          {{- end }}
          http{{ if .Values.tls.enabled }}s{{ end }}://127.0.0.1:9200/_cat/shards?format=json
      )
      SHARD_COUNT=$(
    		python - <<-EOF
    			import json
    			import os
    
    			obj = json.loads(os.environ.get('SHARD_STATS'))
    			count = len(filter(lambda datum: datum['node'] == os.environ.get('HOSTNAME'), obj))
    			print(count)
    		EOF
      )
      echo "Node contains ${SHARD_COUNT} shards"
      sleep 1
    done

    echo "Node clear to shutdown"
